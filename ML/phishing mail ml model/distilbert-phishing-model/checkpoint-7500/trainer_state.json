{
  "best_global_step": 7500,
  "best_metric": 0.02408156357705593,
  "best_model_checkpoint": "./distilbert-phishing-model\\checkpoint-7500",
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 7500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0026666666666666666,
      "grad_norm": 0.0002907139132730663,
      "learning_rate": 4.9873333333333336e-05,
      "loss": 0.0359,
      "step": 20
    },
    {
      "epoch": 0.005333333333333333,
      "grad_norm": 0.025252120569348335,
      "learning_rate": 4.974e-05,
      "loss": 0.0527,
      "step": 40
    },
    {
      "epoch": 0.008,
      "grad_norm": 0.007791158277541399,
      "learning_rate": 4.960666666666667e-05,
      "loss": 0.0395,
      "step": 60
    },
    {
      "epoch": 0.010666666666666666,
      "grad_norm": 0.006954868324100971,
      "learning_rate": 4.947333333333334e-05,
      "loss": 0.0003,
      "step": 80
    },
    {
      "epoch": 0.013333333333333334,
      "grad_norm": Infinity,
      "learning_rate": 4.9340000000000005e-05,
      "loss": 0.0071,
      "step": 100
    },
    {
      "epoch": 0.016,
      "grad_norm": 0.0024595444556325674,
      "learning_rate": 4.921333333333333e-05,
      "loss": 0.0161,
      "step": 120
    },
    {
      "epoch": 0.018666666666666668,
      "grad_norm": 0.0017327074892818928,
      "learning_rate": 4.908666666666667e-05,
      "loss": 0.0742,
      "step": 140
    },
    {
      "epoch": 0.021333333333333333,
      "grad_norm": 0.0865863561630249,
      "learning_rate": 4.896e-05,
      "loss": 0.0313,
      "step": 160
    },
    {
      "epoch": 0.024,
      "grad_norm": 0.10980547964572906,
      "learning_rate": 4.882666666666667e-05,
      "loss": 0.0616,
      "step": 180
    },
    {
      "epoch": 0.02666666666666667,
      "grad_norm": 0.013747446238994598,
      "learning_rate": 4.869333333333334e-05,
      "loss": 0.039,
      "step": 200
    },
    {
      "epoch": 0.029333333333333333,
      "grad_norm": 0.008879651315510273,
      "learning_rate": 4.856e-05,
      "loss": 0.0183,
      "step": 220
    },
    {
      "epoch": 0.032,
      "grad_norm": 0.0023166730534285307,
      "learning_rate": 4.842666666666667e-05,
      "loss": 0.0016,
      "step": 240
    },
    {
      "epoch": 0.034666666666666665,
      "grad_norm": 17.63784408569336,
      "learning_rate": 4.8293333333333334e-05,
      "loss": 0.0405,
      "step": 260
    },
    {
      "epoch": 0.037333333333333336,
      "grad_norm": 0.07595343887805939,
      "learning_rate": 4.816e-05,
      "loss": 0.016,
      "step": 280
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.009612416848540306,
      "learning_rate": 4.802666666666667e-05,
      "loss": 0.0376,
      "step": 300
    },
    {
      "epoch": 0.042666666666666665,
      "grad_norm": 0.004714996553957462,
      "learning_rate": 4.789333333333334e-05,
      "loss": 0.0227,
      "step": 320
    },
    {
      "epoch": 0.04533333333333334,
      "grad_norm": 0.017388230189681053,
      "learning_rate": 4.7760000000000004e-05,
      "loss": 0.0586,
      "step": 340
    },
    {
      "epoch": 0.048,
      "grad_norm": 0.021737201139330864,
      "learning_rate": 4.762666666666667e-05,
      "loss": 0.0534,
      "step": 360
    },
    {
      "epoch": 0.050666666666666665,
      "grad_norm": 0.01673141121864319,
      "learning_rate": 4.7493333333333335e-05,
      "loss": 0.0078,
      "step": 380
    },
    {
      "epoch": 0.05333333333333334,
      "grad_norm": 0.19917145371437073,
      "learning_rate": 4.736000000000001e-05,
      "loss": 0.0477,
      "step": 400
    },
    {
      "epoch": 0.056,
      "grad_norm": 0.008780209347605705,
      "learning_rate": 4.7226666666666666e-05,
      "loss": 0.0073,
      "step": 420
    },
    {
      "epoch": 0.058666666666666666,
      "grad_norm": 0.014937205240130424,
      "learning_rate": 4.709333333333333e-05,
      "loss": 0.0182,
      "step": 440
    },
    {
      "epoch": 0.06133333333333333,
      "grad_norm": 0.018841208890080452,
      "learning_rate": 4.6960000000000004e-05,
      "loss": 0.0103,
      "step": 460
    },
    {
      "epoch": 0.064,
      "grad_norm": 0.0031927081290632486,
      "learning_rate": 4.682666666666667e-05,
      "loss": 0.0069,
      "step": 480
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 0.015166133642196655,
      "learning_rate": 4.6693333333333336e-05,
      "loss": 0.0153,
      "step": 500
    },
    {
      "epoch": 0.06933333333333333,
      "grad_norm": 0.015920300036668777,
      "learning_rate": 4.656e-05,
      "loss": 0.0123,
      "step": 520
    },
    {
      "epoch": 0.072,
      "grad_norm": 0.0031479867175221443,
      "learning_rate": 4.642666666666667e-05,
      "loss": 0.0292,
      "step": 540
    },
    {
      "epoch": 0.07466666666666667,
      "grad_norm": 0.00285292393527925,
      "learning_rate": 4.629333333333333e-05,
      "loss": 0.0297,
      "step": 560
    },
    {
      "epoch": 0.07733333333333334,
      "grad_norm": 0.003957741893827915,
      "learning_rate": 4.6160000000000005e-05,
      "loss": 0.0225,
      "step": 580
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.012581830844283104,
      "learning_rate": 4.602666666666667e-05,
      "loss": 0.0329,
      "step": 600
    },
    {
      "epoch": 0.08266666666666667,
      "grad_norm": 0.02880999632179737,
      "learning_rate": 4.589333333333334e-05,
      "loss": 0.0212,
      "step": 620
    },
    {
      "epoch": 0.08533333333333333,
      "grad_norm": 0.010160411708056927,
      "learning_rate": 4.576e-05,
      "loss": 0.001,
      "step": 640
    },
    {
      "epoch": 0.088,
      "grad_norm": 0.012764797545969486,
      "learning_rate": 4.562666666666667e-05,
      "loss": 0.0306,
      "step": 660
    },
    {
      "epoch": 0.09066666666666667,
      "grad_norm": 0.07301148772239685,
      "learning_rate": 4.549333333333334e-05,
      "loss": 0.0186,
      "step": 680
    },
    {
      "epoch": 0.09333333333333334,
      "grad_norm": 0.12629206478595734,
      "learning_rate": 4.536e-05,
      "loss": 0.0036,
      "step": 700
    },
    {
      "epoch": 0.096,
      "grad_norm": 0.00969669409096241,
      "learning_rate": 4.5226666666666665e-05,
      "loss": 0.0352,
      "step": 720
    },
    {
      "epoch": 0.09866666666666667,
      "grad_norm": 0.013399835675954819,
      "learning_rate": 4.509333333333334e-05,
      "loss": 0.0014,
      "step": 740
    },
    {
      "epoch": 0.10133333333333333,
      "grad_norm": 0.001928288722410798,
      "learning_rate": 4.496e-05,
      "loss": 0.0018,
      "step": 760
    },
    {
      "epoch": 0.104,
      "grad_norm": 2.858361005783081,
      "learning_rate": 4.482666666666667e-05,
      "loss": 0.045,
      "step": 780
    },
    {
      "epoch": 0.10666666666666667,
      "grad_norm": 0.011712750419974327,
      "learning_rate": 4.4693333333333335e-05,
      "loss": 0.0372,
      "step": 800
    },
    {
      "epoch": 0.10933333333333334,
      "grad_norm": 0.011655638925731182,
      "learning_rate": 4.456e-05,
      "loss": 0.036,
      "step": 820
    },
    {
      "epoch": 0.112,
      "grad_norm": 0.021202607080340385,
      "learning_rate": 4.443333333333334e-05,
      "loss": 0.0368,
      "step": 840
    },
    {
      "epoch": 0.11466666666666667,
      "grad_norm": 0.015680722892284393,
      "learning_rate": 4.43e-05,
      "loss": 0.019,
      "step": 860
    },
    {
      "epoch": 0.11733333333333333,
      "grad_norm": 0.03951926529407501,
      "learning_rate": 4.4166666666666665e-05,
      "loss": 0.0255,
      "step": 880
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.01411107275635004,
      "learning_rate": 4.403333333333334e-05,
      "loss": 0.0118,
      "step": 900
    },
    {
      "epoch": 0.12266666666666666,
      "grad_norm": 0.01141401007771492,
      "learning_rate": 4.39e-05,
      "loss": 0.0342,
      "step": 920
    },
    {
      "epoch": 0.12533333333333332,
      "grad_norm": 0.009658919647336006,
      "learning_rate": 4.376666666666667e-05,
      "loss": 0.0413,
      "step": 940
    },
    {
      "epoch": 0.128,
      "grad_norm": 1.6666259765625,
      "learning_rate": 4.3633333333333335e-05,
      "loss": 0.0458,
      "step": 960
    },
    {
      "epoch": 0.13066666666666665,
      "grad_norm": 0.06524348258972168,
      "learning_rate": 4.35e-05,
      "loss": 0.0025,
      "step": 980
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 0.011606029234826565,
      "learning_rate": 4.3366666666666666e-05,
      "loss": 0.0272,
      "step": 1000
    },
    {
      "epoch": 0.136,
      "grad_norm": 0.048592403531074524,
      "learning_rate": 4.323333333333334e-05,
      "loss": 0.0184,
      "step": 1020
    },
    {
      "epoch": 0.13866666666666666,
      "grad_norm": 0.0336557999253273,
      "learning_rate": 4.3100000000000004e-05,
      "loss": 0.002,
      "step": 1040
    },
    {
      "epoch": 0.14133333333333334,
      "grad_norm": 0.010277889668941498,
      "learning_rate": 4.296666666666666e-05,
      "loss": 0.0014,
      "step": 1060
    },
    {
      "epoch": 0.144,
      "grad_norm": 0.022611193358898163,
      "learning_rate": 4.2833333333333335e-05,
      "loss": 0.0224,
      "step": 1080
    },
    {
      "epoch": 0.14666666666666667,
      "grad_norm": 0.02502872422337532,
      "learning_rate": 4.27e-05,
      "loss": 0.0009,
      "step": 1100
    },
    {
      "epoch": 0.14933333333333335,
      "grad_norm": 0.03674708679318428,
      "learning_rate": 4.2566666666666674e-05,
      "loss": 0.0464,
      "step": 1120
    },
    {
      "epoch": 0.152,
      "grad_norm": 0.005746596958488226,
      "learning_rate": 4.243333333333334e-05,
      "loss": 0.0013,
      "step": 1140
    },
    {
      "epoch": 0.15466666666666667,
      "grad_norm": 0.005287577863782644,
      "learning_rate": 4.23e-05,
      "loss": 0.007,
      "step": 1160
    },
    {
      "epoch": 0.15733333333333333,
      "grad_norm": 0.019654948264360428,
      "learning_rate": 4.216666666666667e-05,
      "loss": 0.029,
      "step": 1180
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.09697526693344116,
      "learning_rate": 4.2033333333333336e-05,
      "loss": 0.0225,
      "step": 1200
    },
    {
      "epoch": 0.16266666666666665,
      "grad_norm": 0.5813341736793518,
      "learning_rate": 4.19e-05,
      "loss": 0.0063,
      "step": 1220
    },
    {
      "epoch": 0.16533333333333333,
      "grad_norm": 0.010754445567727089,
      "learning_rate": 4.176666666666667e-05,
      "loss": 0.0259,
      "step": 1240
    },
    {
      "epoch": 0.168,
      "grad_norm": 0.007308704778552055,
      "learning_rate": 4.1633333333333333e-05,
      "loss": 0.0055,
      "step": 1260
    },
    {
      "epoch": 0.17066666666666666,
      "grad_norm": 0.016112111508846283,
      "learning_rate": 4.15e-05,
      "loss": 0.0256,
      "step": 1280
    },
    {
      "epoch": 0.17333333333333334,
      "grad_norm": 0.016774127259850502,
      "learning_rate": 4.136666666666667e-05,
      "loss": 0.049,
      "step": 1300
    },
    {
      "epoch": 0.176,
      "grad_norm": 0.011079461313784122,
      "learning_rate": 4.123333333333334e-05,
      "loss": 0.0084,
      "step": 1320
    },
    {
      "epoch": 0.17866666666666667,
      "grad_norm": 0.029204783961176872,
      "learning_rate": 4.11e-05,
      "loss": 0.0005,
      "step": 1340
    },
    {
      "epoch": 0.18133333333333335,
      "grad_norm": 0.011966652236878872,
      "learning_rate": 4.096666666666667e-05,
      "loss": 0.0257,
      "step": 1360
    },
    {
      "epoch": 0.184,
      "grad_norm": 0.023149680346250534,
      "learning_rate": 4.0833333333333334e-05,
      "loss": 0.0184,
      "step": 1380
    },
    {
      "epoch": 0.18666666666666668,
      "grad_norm": 0.006051354575902224,
      "learning_rate": 4.07e-05,
      "loss": 0.0021,
      "step": 1400
    },
    {
      "epoch": 0.18933333333333333,
      "grad_norm": 0.07475914061069489,
      "learning_rate": 4.056666666666667e-05,
      "loss": 0.0695,
      "step": 1420
    },
    {
      "epoch": 0.192,
      "grad_norm": 0.10886340588331223,
      "learning_rate": 4.043333333333333e-05,
      "loss": 0.0415,
      "step": 1440
    },
    {
      "epoch": 0.19466666666666665,
      "grad_norm": 10.18606185913086,
      "learning_rate": 4.0300000000000004e-05,
      "loss": 0.0509,
      "step": 1460
    },
    {
      "epoch": 0.19733333333333333,
      "grad_norm": 0.5236654281616211,
      "learning_rate": 4.016666666666667e-05,
      "loss": 0.0101,
      "step": 1480
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.0741962417960167,
      "learning_rate": 4.0033333333333335e-05,
      "loss": 0.0112,
      "step": 1500
    },
    {
      "epoch": 0.20266666666666666,
      "grad_norm": 0.2770128846168518,
      "learning_rate": 3.99e-05,
      "loss": 0.0293,
      "step": 1520
    },
    {
      "epoch": 0.20533333333333334,
      "grad_norm": 0.019574085250496864,
      "learning_rate": 3.9766666666666667e-05,
      "loss": 0.0183,
      "step": 1540
    },
    {
      "epoch": 0.208,
      "grad_norm": 0.0225132517516613,
      "learning_rate": 3.963333333333333e-05,
      "loss": 0.0486,
      "step": 1560
    },
    {
      "epoch": 0.21066666666666667,
      "grad_norm": 0.015661392360925674,
      "learning_rate": 3.9500000000000005e-05,
      "loss": 0.001,
      "step": 1580
    },
    {
      "epoch": 0.21333333333333335,
      "grad_norm": 0.00617667380720377,
      "learning_rate": 3.936666666666667e-05,
      "loss": 0.0019,
      "step": 1600
    },
    {
      "epoch": 0.216,
      "grad_norm": 0.008734805509448051,
      "learning_rate": 3.9233333333333336e-05,
      "loss": 0.0416,
      "step": 1620
    },
    {
      "epoch": 0.21866666666666668,
      "grad_norm": 2.2937448024749756,
      "learning_rate": 3.91e-05,
      "loss": 0.0845,
      "step": 1640
    },
    {
      "epoch": 0.22133333333333333,
      "grad_norm": 0.04612692445516586,
      "learning_rate": 3.896666666666667e-05,
      "loss": 0.003,
      "step": 1660
    },
    {
      "epoch": 0.224,
      "grad_norm": 1.2968610525131226,
      "learning_rate": 3.883333333333333e-05,
      "loss": 0.0754,
      "step": 1680
    },
    {
      "epoch": 0.22666666666666666,
      "grad_norm": 0.3189767599105835,
      "learning_rate": 3.8700000000000006e-05,
      "loss": 0.0305,
      "step": 1700
    },
    {
      "epoch": 0.22933333333333333,
      "grad_norm": 0.013591607101261616,
      "learning_rate": 3.8566666666666664e-05,
      "loss": 0.03,
      "step": 1720
    },
    {
      "epoch": 0.232,
      "grad_norm": 0.00815693847835064,
      "learning_rate": 3.843333333333334e-05,
      "loss": 0.0247,
      "step": 1740
    },
    {
      "epoch": 0.23466666666666666,
      "grad_norm": 0.03481067344546318,
      "learning_rate": 3.83e-05,
      "loss": 0.0609,
      "step": 1760
    },
    {
      "epoch": 0.23733333333333334,
      "grad_norm": 0.033675018697977066,
      "learning_rate": 3.816666666666667e-05,
      "loss": 0.0336,
      "step": 1780
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.02101690135896206,
      "learning_rate": 3.803333333333334e-05,
      "loss": 0.0242,
      "step": 1800
    },
    {
      "epoch": 0.24266666666666667,
      "grad_norm": 0.018712608143687248,
      "learning_rate": 3.79e-05,
      "loss": 0.0231,
      "step": 1820
    },
    {
      "epoch": 0.24533333333333332,
      "grad_norm": 0.01817105896770954,
      "learning_rate": 3.7766666666666665e-05,
      "loss": 0.0122,
      "step": 1840
    },
    {
      "epoch": 0.248,
      "grad_norm": 0.010994097217917442,
      "learning_rate": 3.763333333333334e-05,
      "loss": 0.0242,
      "step": 1860
    },
    {
      "epoch": 0.25066666666666665,
      "grad_norm": 0.029366156086325645,
      "learning_rate": 3.7500000000000003e-05,
      "loss": 0.0184,
      "step": 1880
    },
    {
      "epoch": 0.25333333333333335,
      "grad_norm": 0.01912524364888668,
      "learning_rate": 3.736666666666667e-05,
      "loss": 0.0478,
      "step": 1900
    },
    {
      "epoch": 0.256,
      "grad_norm": 13.004340171813965,
      "learning_rate": 3.7233333333333335e-05,
      "loss": 0.029,
      "step": 1920
    },
    {
      "epoch": 0.25866666666666666,
      "grad_norm": 0.04580839350819588,
      "learning_rate": 3.71e-05,
      "loss": 0.0463,
      "step": 1940
    },
    {
      "epoch": 0.2613333333333333,
      "grad_norm": 0.030833106487989426,
      "learning_rate": 3.6966666666666666e-05,
      "loss": 0.0145,
      "step": 1960
    },
    {
      "epoch": 0.264,
      "grad_norm": 0.023612504824995995,
      "learning_rate": 3.683333333333334e-05,
      "loss": 0.0223,
      "step": 1980
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 0.03462207317352295,
      "learning_rate": 3.6700000000000004e-05,
      "loss": 0.002,
      "step": 2000
    },
    {
      "epoch": 0.2693333333333333,
      "grad_norm": 0.7370848655700684,
      "learning_rate": 3.656666666666666e-05,
      "loss": 0.0263,
      "step": 2020
    },
    {
      "epoch": 0.272,
      "grad_norm": 0.02563157118856907,
      "learning_rate": 3.6433333333333336e-05,
      "loss": 0.0074,
      "step": 2040
    },
    {
      "epoch": 0.27466666666666667,
      "grad_norm": 0.3892994821071625,
      "learning_rate": 3.63e-05,
      "loss": 0.049,
      "step": 2060
    },
    {
      "epoch": 0.2773333333333333,
      "grad_norm": 0.030862189829349518,
      "learning_rate": 3.6166666666666674e-05,
      "loss": 0.0062,
      "step": 2080
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.04942901059985161,
      "learning_rate": 3.603333333333333e-05,
      "loss": 0.0417,
      "step": 2100
    },
    {
      "epoch": 0.2826666666666667,
      "grad_norm": 0.019970305263996124,
      "learning_rate": 3.59e-05,
      "loss": 0.0043,
      "step": 2120
    },
    {
      "epoch": 0.2853333333333333,
      "grad_norm": 0.017786666750907898,
      "learning_rate": 3.576666666666667e-05,
      "loss": 0.0246,
      "step": 2140
    },
    {
      "epoch": 0.288,
      "grad_norm": 0.007865573279559612,
      "learning_rate": 3.563333333333334e-05,
      "loss": 0.0205,
      "step": 2160
    },
    {
      "epoch": 0.2906666666666667,
      "grad_norm": 0.014146475121378899,
      "learning_rate": 3.55e-05,
      "loss": 0.0018,
      "step": 2180
    },
    {
      "epoch": 0.29333333333333333,
      "grad_norm": 0.01835392415523529,
      "learning_rate": 3.536666666666667e-05,
      "loss": 0.0187,
      "step": 2200
    },
    {
      "epoch": 0.296,
      "grad_norm": 0.027697686105966568,
      "learning_rate": 3.5233333333333334e-05,
      "loss": 0.0048,
      "step": 2220
    },
    {
      "epoch": 0.2986666666666667,
      "grad_norm": 0.013512521982192993,
      "learning_rate": 3.51e-05,
      "loss": 0.0073,
      "step": 2240
    },
    {
      "epoch": 0.30133333333333334,
      "grad_norm": 3.2981858253479004,
      "learning_rate": 3.496666666666667e-05,
      "loss": 0.0205,
      "step": 2260
    },
    {
      "epoch": 0.304,
      "grad_norm": 0.07837585359811783,
      "learning_rate": 3.483333333333334e-05,
      "loss": 0.0656,
      "step": 2280
    },
    {
      "epoch": 0.30666666666666664,
      "grad_norm": 0.035038236528635025,
      "learning_rate": 3.4699999999999996e-05,
      "loss": 0.0018,
      "step": 2300
    },
    {
      "epoch": 0.30933333333333335,
      "grad_norm": 0.794502854347229,
      "learning_rate": 3.456666666666667e-05,
      "loss": 0.0041,
      "step": 2320
    },
    {
      "epoch": 0.312,
      "grad_norm": 0.01270782295614481,
      "learning_rate": 3.4433333333333335e-05,
      "loss": 0.0093,
      "step": 2340
    },
    {
      "epoch": 0.31466666666666665,
      "grad_norm": 0.01186259277164936,
      "learning_rate": 3.430000000000001e-05,
      "loss": 0.0003,
      "step": 2360
    },
    {
      "epoch": 0.31733333333333336,
      "grad_norm": 0.010825072415173054,
      "learning_rate": 3.4166666666666666e-05,
      "loss": 0.0294,
      "step": 2380
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.017212288454174995,
      "learning_rate": 3.403333333333333e-05,
      "loss": 0.052,
      "step": 2400
    },
    {
      "epoch": 0.32266666666666666,
      "grad_norm": 0.008914091624319553,
      "learning_rate": 3.3900000000000004e-05,
      "loss": 0.0005,
      "step": 2420
    },
    {
      "epoch": 0.3253333333333333,
      "grad_norm": 0.010120273567736149,
      "learning_rate": 3.376666666666667e-05,
      "loss": 0.0004,
      "step": 2440
    },
    {
      "epoch": 0.328,
      "grad_norm": 0.0063820043578743935,
      "learning_rate": 3.3633333333333335e-05,
      "loss": 0.0031,
      "step": 2460
    },
    {
      "epoch": 0.33066666666666666,
      "grad_norm": 0.004445117898285389,
      "learning_rate": 3.35e-05,
      "loss": 0.0042,
      "step": 2480
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 0.006816501263529062,
      "learning_rate": 3.336666666666667e-05,
      "loss": 0.0002,
      "step": 2500
    },
    {
      "epoch": 0.336,
      "grad_norm": 0.8086022138595581,
      "learning_rate": 3.323333333333333e-05,
      "loss": 0.0068,
      "step": 2520
    },
    {
      "epoch": 0.33866666666666667,
      "grad_norm": 0.3984159529209137,
      "learning_rate": 3.3100000000000005e-05,
      "loss": 0.0291,
      "step": 2540
    },
    {
      "epoch": 0.3413333333333333,
      "grad_norm": 0.12093641608953476,
      "learning_rate": 3.296666666666667e-05,
      "loss": 0.0507,
      "step": 2560
    },
    {
      "epoch": 0.344,
      "grad_norm": 0.01129145361483097,
      "learning_rate": 3.283333333333333e-05,
      "loss": 0.0148,
      "step": 2580
    },
    {
      "epoch": 0.3466666666666667,
      "grad_norm": 0.01898133009672165,
      "learning_rate": 3.27e-05,
      "loss": 0.0021,
      "step": 2600
    },
    {
      "epoch": 0.34933333333333333,
      "grad_norm": 0.007273826282471418,
      "learning_rate": 3.256666666666667e-05,
      "loss": 0.0064,
      "step": 2620
    },
    {
      "epoch": 0.352,
      "grad_norm": 0.006812842097133398,
      "learning_rate": 3.243333333333333e-05,
      "loss": 0.0099,
      "step": 2640
    },
    {
      "epoch": 0.3546666666666667,
      "grad_norm": 0.014933335594832897,
      "learning_rate": 3.2300000000000006e-05,
      "loss": 0.0003,
      "step": 2660
    },
    {
      "epoch": 0.35733333333333334,
      "grad_norm": 0.0048975870013237,
      "learning_rate": 3.2166666666666665e-05,
      "loss": 0.0074,
      "step": 2680
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.07130438089370728,
      "learning_rate": 3.203333333333334e-05,
      "loss": 0.0157,
      "step": 2700
    },
    {
      "epoch": 0.3626666666666667,
      "grad_norm": 0.00318534136749804,
      "learning_rate": 3.19e-05,
      "loss": 0.0038,
      "step": 2720
    },
    {
      "epoch": 0.36533333333333334,
      "grad_norm": 0.011374350637197495,
      "learning_rate": 3.176666666666667e-05,
      "loss": 0.0296,
      "step": 2740
    },
    {
      "epoch": 0.368,
      "grad_norm": 0.009315439499914646,
      "learning_rate": 3.1633333333333334e-05,
      "loss": 0.0175,
      "step": 2760
    },
    {
      "epoch": 0.37066666666666664,
      "grad_norm": 0.007891117595136166,
      "learning_rate": 3.15e-05,
      "loss": 0.0301,
      "step": 2780
    },
    {
      "epoch": 0.37333333333333335,
      "grad_norm": 0.0067681847140192986,
      "learning_rate": 3.1366666666666666e-05,
      "loss": 0.0007,
      "step": 2800
    },
    {
      "epoch": 0.376,
      "grad_norm": 0.018501272425055504,
      "learning_rate": 3.123333333333334e-05,
      "loss": 0.0161,
      "step": 2820
    },
    {
      "epoch": 0.37866666666666665,
      "grad_norm": 0.01039027888327837,
      "learning_rate": 3.1100000000000004e-05,
      "loss": 0.0089,
      "step": 2840
    },
    {
      "epoch": 0.38133333333333336,
      "grad_norm": 0.00810092780739069,
      "learning_rate": 3.096666666666666e-05,
      "loss": 0.0095,
      "step": 2860
    },
    {
      "epoch": 0.384,
      "grad_norm": 0.0059869056567549706,
      "learning_rate": 3.0833333333333335e-05,
      "loss": 0.0173,
      "step": 2880
    },
    {
      "epoch": 0.38666666666666666,
      "grad_norm": 0.002681740326806903,
      "learning_rate": 3.07e-05,
      "loss": 0.0027,
      "step": 2900
    },
    {
      "epoch": 0.3893333333333333,
      "grad_norm": 12.783279418945312,
      "learning_rate": 3.0566666666666667e-05,
      "loss": 0.0234,
      "step": 2920
    },
    {
      "epoch": 0.392,
      "grad_norm": 0.012483355589210987,
      "learning_rate": 3.0433333333333336e-05,
      "loss": 0.0283,
      "step": 2940
    },
    {
      "epoch": 0.39466666666666667,
      "grad_norm": 93.98808288574219,
      "learning_rate": 3.03e-05,
      "loss": 0.0428,
      "step": 2960
    },
    {
      "epoch": 0.3973333333333333,
      "grad_norm": 0.22995920479297638,
      "learning_rate": 3.016666666666667e-05,
      "loss": 0.0415,
      "step": 2980
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.01444148737937212,
      "learning_rate": 3.0033333333333336e-05,
      "loss": 0.0253,
      "step": 3000
    },
    {
      "epoch": 0.4026666666666667,
      "grad_norm": 0.016961347311735153,
      "learning_rate": 2.9900000000000002e-05,
      "loss": 0.0053,
      "step": 3020
    },
    {
      "epoch": 0.4053333333333333,
      "grad_norm": 0.07598903030157089,
      "learning_rate": 2.976666666666667e-05,
      "loss": 0.0199,
      "step": 3040
    },
    {
      "epoch": 0.408,
      "grad_norm": 0.0050618271343410015,
      "learning_rate": 2.9633333333333336e-05,
      "loss": 0.002,
      "step": 3060
    },
    {
      "epoch": 0.4106666666666667,
      "grad_norm": 0.008341878652572632,
      "learning_rate": 2.95e-05,
      "loss": 0.0069,
      "step": 3080
    },
    {
      "epoch": 0.41333333333333333,
      "grad_norm": 0.004116905387490988,
      "learning_rate": 2.936666666666667e-05,
      "loss": 0.0208,
      "step": 3100
    },
    {
      "epoch": 0.416,
      "grad_norm": 0.0028410139493644238,
      "learning_rate": 2.9233333333333334e-05,
      "loss": 0.0006,
      "step": 3120
    },
    {
      "epoch": 0.4186666666666667,
      "grad_norm": 0.0022565065883100033,
      "learning_rate": 2.91e-05,
      "loss": 0.0636,
      "step": 3140
    },
    {
      "epoch": 0.42133333333333334,
      "grad_norm": 0.035308461636304855,
      "learning_rate": 2.8966666666666668e-05,
      "loss": 0.0764,
      "step": 3160
    },
    {
      "epoch": 0.424,
      "grad_norm": 0.2727065682411194,
      "learning_rate": 2.8833333333333334e-05,
      "loss": 0.0091,
      "step": 3180
    },
    {
      "epoch": 0.4266666666666667,
      "grad_norm": 0.006861022673547268,
      "learning_rate": 2.87e-05,
      "loss": 0.0003,
      "step": 3200
    },
    {
      "epoch": 0.42933333333333334,
      "grad_norm": 0.003822298953309655,
      "learning_rate": 2.856666666666667e-05,
      "loss": 0.0015,
      "step": 3220
    },
    {
      "epoch": 0.432,
      "grad_norm": 0.2083246260881424,
      "learning_rate": 2.8433333333333334e-05,
      "loss": 0.0018,
      "step": 3240
    },
    {
      "epoch": 0.43466666666666665,
      "grad_norm": 0.0063281250186264515,
      "learning_rate": 2.83e-05,
      "loss": 0.0193,
      "step": 3260
    },
    {
      "epoch": 0.43733333333333335,
      "grad_norm": 1.158047080039978,
      "learning_rate": 2.816666666666667e-05,
      "loss": 0.0457,
      "step": 3280
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.2940348982810974,
      "learning_rate": 2.8033333333333335e-05,
      "loss": 0.0075,
      "step": 3300
    },
    {
      "epoch": 0.44266666666666665,
      "grad_norm": 0.0035381808411329985,
      "learning_rate": 2.7900000000000004e-05,
      "loss": 0.0307,
      "step": 3320
    },
    {
      "epoch": 0.44533333333333336,
      "grad_norm": 0.006637249607592821,
      "learning_rate": 2.776666666666667e-05,
      "loss": 0.0008,
      "step": 3340
    },
    {
      "epoch": 0.448,
      "grad_norm": 0.008239686489105225,
      "learning_rate": 2.7633333333333332e-05,
      "loss": 0.0273,
      "step": 3360
    },
    {
      "epoch": 0.45066666666666666,
      "grad_norm": 0.0061861551366746426,
      "learning_rate": 2.7500000000000004e-05,
      "loss": 0.0006,
      "step": 3380
    },
    {
      "epoch": 0.4533333333333333,
      "grad_norm": 0.01939985156059265,
      "learning_rate": 2.7366666666666667e-05,
      "loss": 0.0366,
      "step": 3400
    },
    {
      "epoch": 0.456,
      "grad_norm": 0.03782772272825241,
      "learning_rate": 2.7233333333333332e-05,
      "loss": 0.0191,
      "step": 3420
    },
    {
      "epoch": 0.45866666666666667,
      "grad_norm": 0.012255215086042881,
      "learning_rate": 2.7100000000000005e-05,
      "loss": 0.0015,
      "step": 3440
    },
    {
      "epoch": 0.4613333333333333,
      "grad_norm": 7.583488464355469,
      "learning_rate": 2.6966666666666667e-05,
      "loss": 0.0132,
      "step": 3460
    },
    {
      "epoch": 0.464,
      "grad_norm": 0.004862598143517971,
      "learning_rate": 2.6833333333333333e-05,
      "loss": 0.0012,
      "step": 3480
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 0.0034053497947752476,
      "learning_rate": 2.6700000000000002e-05,
      "loss": 0.0019,
      "step": 3500
    },
    {
      "epoch": 0.4693333333333333,
      "grad_norm": 0.014032432809472084,
      "learning_rate": 2.6566666666666668e-05,
      "loss": 0.0321,
      "step": 3520
    },
    {
      "epoch": 0.472,
      "grad_norm": 0.005443092435598373,
      "learning_rate": 2.6433333333333333e-05,
      "loss": 0.036,
      "step": 3540
    },
    {
      "epoch": 0.4746666666666667,
      "grad_norm": 0.008185336366295815,
      "learning_rate": 2.6300000000000002e-05,
      "loss": 0.0126,
      "step": 3560
    },
    {
      "epoch": 0.47733333333333333,
      "grad_norm": 0.3447364568710327,
      "learning_rate": 2.6166666666666668e-05,
      "loss": 0.0114,
      "step": 3580
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.011199845932424068,
      "learning_rate": 2.6033333333333337e-05,
      "loss": 0.001,
      "step": 3600
    },
    {
      "epoch": 0.4826666666666667,
      "grad_norm": 0.0036668083630502224,
      "learning_rate": 2.5900000000000003e-05,
      "loss": 0.0018,
      "step": 3620
    },
    {
      "epoch": 0.48533333333333334,
      "grad_norm": 0.003766156965866685,
      "learning_rate": 2.5766666666666665e-05,
      "loss": 0.0017,
      "step": 3640
    },
    {
      "epoch": 0.488,
      "grad_norm": 0.0032864785753190517,
      "learning_rate": 2.5633333333333338e-05,
      "loss": 0.0105,
      "step": 3660
    },
    {
      "epoch": 0.49066666666666664,
      "grad_norm": 0.0025254273787140846,
      "learning_rate": 2.5500000000000003e-05,
      "loss": 0.0001,
      "step": 3680
    },
    {
      "epoch": 0.49333333333333335,
      "grad_norm": 0.003003830322995782,
      "learning_rate": 2.5366666666666665e-05,
      "loss": 0.0053,
      "step": 3700
    },
    {
      "epoch": 0.496,
      "grad_norm": 0.00832123588770628,
      "learning_rate": 2.5233333333333338e-05,
      "loss": 0.0304,
      "step": 3720
    },
    {
      "epoch": 0.49866666666666665,
      "grad_norm": 0.010351274162530899,
      "learning_rate": 2.51e-05,
      "loss": 0.0004,
      "step": 3740
    },
    {
      "epoch": 0.5013333333333333,
      "grad_norm": 9.103896141052246,
      "learning_rate": 2.496666666666667e-05,
      "loss": 0.0376,
      "step": 3760
    },
    {
      "epoch": 0.504,
      "grad_norm": 0.009791277348995209,
      "learning_rate": 2.4833333333333335e-05,
      "loss": 0.0095,
      "step": 3780
    },
    {
      "epoch": 0.5066666666666667,
      "grad_norm": 0.017475023865699768,
      "learning_rate": 2.47e-05,
      "loss": 0.0496,
      "step": 3800
    },
    {
      "epoch": 0.5093333333333333,
      "grad_norm": 0.028702370822429657,
      "learning_rate": 2.456666666666667e-05,
      "loss": 0.0062,
      "step": 3820
    },
    {
      "epoch": 0.512,
      "grad_norm": 0.010157711803913116,
      "learning_rate": 2.4433333333333335e-05,
      "loss": 0.024,
      "step": 3840
    },
    {
      "epoch": 0.5146666666666667,
      "grad_norm": 0.019694171845912933,
      "learning_rate": 2.43e-05,
      "loss": 0.0008,
      "step": 3860
    },
    {
      "epoch": 0.5173333333333333,
      "grad_norm": 0.010408326983451843,
      "learning_rate": 2.4166666666666667e-05,
      "loss": 0.0162,
      "step": 3880
    },
    {
      "epoch": 0.52,
      "grad_norm": 2.4086246490478516,
      "learning_rate": 2.4033333333333336e-05,
      "loss": 0.0308,
      "step": 3900
    },
    {
      "epoch": 0.5226666666666666,
      "grad_norm": 0.021869614720344543,
      "learning_rate": 2.39e-05,
      "loss": 0.0006,
      "step": 3920
    },
    {
      "epoch": 0.5253333333333333,
      "grad_norm": 0.6704086065292358,
      "learning_rate": 2.3766666666666667e-05,
      "loss": 0.0412,
      "step": 3940
    },
    {
      "epoch": 0.528,
      "grad_norm": 0.01364822406321764,
      "learning_rate": 2.3633333333333336e-05,
      "loss": 0.0216,
      "step": 3960
    },
    {
      "epoch": 0.5306666666666666,
      "grad_norm": 0.018601538613438606,
      "learning_rate": 2.35e-05,
      "loss": 0.0256,
      "step": 3980
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 0.01480390876531601,
      "learning_rate": 2.3366666666666668e-05,
      "loss": 0.0189,
      "step": 4000
    },
    {
      "epoch": 0.536,
      "grad_norm": 0.05303849279880524,
      "learning_rate": 2.3233333333333333e-05,
      "loss": 0.0336,
      "step": 4020
    },
    {
      "epoch": 0.5386666666666666,
      "grad_norm": 0.01896415278315544,
      "learning_rate": 2.3100000000000002e-05,
      "loss": 0.0007,
      "step": 4040
    },
    {
      "epoch": 0.5413333333333333,
      "grad_norm": 0.01222116407006979,
      "learning_rate": 2.2966666666666668e-05,
      "loss": 0.0063,
      "step": 4060
    },
    {
      "epoch": 0.544,
      "grad_norm": 0.01068035326898098,
      "learning_rate": 2.2833333333333334e-05,
      "loss": 0.0029,
      "step": 4080
    },
    {
      "epoch": 0.5466666666666666,
      "grad_norm": 0.010716739110648632,
      "learning_rate": 2.2700000000000003e-05,
      "loss": 0.0054,
      "step": 4100
    },
    {
      "epoch": 0.5493333333333333,
      "grad_norm": 0.007568347733467817,
      "learning_rate": 2.2566666666666665e-05,
      "loss": 0.0321,
      "step": 4120
    },
    {
      "epoch": 0.552,
      "grad_norm": 0.010203572921454906,
      "learning_rate": 2.2433333333333334e-05,
      "loss": 0.0147,
      "step": 4140
    },
    {
      "epoch": 0.5546666666666666,
      "grad_norm": 0.01745992712676525,
      "learning_rate": 2.23e-05,
      "loss": 0.0221,
      "step": 4160
    },
    {
      "epoch": 0.5573333333333333,
      "grad_norm": 0.0033942742738872766,
      "learning_rate": 2.216666666666667e-05,
      "loss": 0.0015,
      "step": 4180
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.018736619502305984,
      "learning_rate": 2.2033333333333335e-05,
      "loss": 0.0018,
      "step": 4200
    },
    {
      "epoch": 0.5626666666666666,
      "grad_norm": 0.007859820500016212,
      "learning_rate": 2.19e-05,
      "loss": 0.0373,
      "step": 4220
    },
    {
      "epoch": 0.5653333333333334,
      "grad_norm": 0.011890295892953873,
      "learning_rate": 2.176666666666667e-05,
      "loss": 0.0244,
      "step": 4240
    },
    {
      "epoch": 0.568,
      "grad_norm": 0.021635185927152634,
      "learning_rate": 2.1633333333333332e-05,
      "loss": 0.0196,
      "step": 4260
    },
    {
      "epoch": 0.5706666666666667,
      "grad_norm": 0.049760960042476654,
      "learning_rate": 2.15e-05,
      "loss": 0.0216,
      "step": 4280
    },
    {
      "epoch": 0.5733333333333334,
      "grad_norm": 0.0257879588752985,
      "learning_rate": 2.1366666666666667e-05,
      "loss": 0.0408,
      "step": 4300
    },
    {
      "epoch": 0.576,
      "grad_norm": 0.07210157811641693,
      "learning_rate": 2.1233333333333336e-05,
      "loss": 0.0523,
      "step": 4320
    },
    {
      "epoch": 0.5786666666666667,
      "grad_norm": 0.0717231035232544,
      "learning_rate": 2.11e-05,
      "loss": 0.0083,
      "step": 4340
    },
    {
      "epoch": 0.5813333333333334,
      "grad_norm": 0.027305837720632553,
      "learning_rate": 2.0966666666666667e-05,
      "loss": 0.0007,
      "step": 4360
    },
    {
      "epoch": 0.584,
      "grad_norm": 0.013281572610139847,
      "learning_rate": 2.0833333333333336e-05,
      "loss": 0.0255,
      "step": 4380
    },
    {
      "epoch": 0.5866666666666667,
      "grad_norm": 0.005837267730385065,
      "learning_rate": 2.07e-05,
      "loss": 0.0091,
      "step": 4400
    },
    {
      "epoch": 0.5893333333333334,
      "grad_norm": 0.00499099213629961,
      "learning_rate": 2.0566666666666667e-05,
      "loss": 0.0072,
      "step": 4420
    },
    {
      "epoch": 0.592,
      "grad_norm": 2.296661376953125,
      "learning_rate": 2.0433333333333336e-05,
      "loss": 0.0363,
      "step": 4440
    },
    {
      "epoch": 0.5946666666666667,
      "grad_norm": 0.028124256059527397,
      "learning_rate": 2.0300000000000002e-05,
      "loss": 0.002,
      "step": 4460
    },
    {
      "epoch": 0.5973333333333334,
      "grad_norm": 0.007899307645857334,
      "learning_rate": 2.0166666666666668e-05,
      "loss": 0.0172,
      "step": 4480
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.01216042973101139,
      "learning_rate": 2.0033333333333334e-05,
      "loss": 0.0197,
      "step": 4500
    },
    {
      "epoch": 0.6026666666666667,
      "grad_norm": 0.009993152692914009,
      "learning_rate": 1.9900000000000003e-05,
      "loss": 0.0004,
      "step": 4520
    },
    {
      "epoch": 0.6053333333333333,
      "grad_norm": 0.009980917908251286,
      "learning_rate": 1.9766666666666668e-05,
      "loss": 0.0161,
      "step": 4540
    },
    {
      "epoch": 0.608,
      "grad_norm": 0.0075381118804216385,
      "learning_rate": 1.9633333333333334e-05,
      "loss": 0.0163,
      "step": 4560
    },
    {
      "epoch": 0.6106666666666667,
      "grad_norm": 0.009048270992934704,
      "learning_rate": 1.9500000000000003e-05,
      "loss": 0.022,
      "step": 4580
    },
    {
      "epoch": 0.6133333333333333,
      "grad_norm": 0.005492833908647299,
      "learning_rate": 1.9366666666666665e-05,
      "loss": 0.0027,
      "step": 4600
    },
    {
      "epoch": 0.616,
      "grad_norm": 0.005209523253142834,
      "learning_rate": 1.9233333333333334e-05,
      "loss": 0.0189,
      "step": 4620
    },
    {
      "epoch": 0.6186666666666667,
      "grad_norm": 3.136287212371826,
      "learning_rate": 1.91e-05,
      "loss": 0.0058,
      "step": 4640
    },
    {
      "epoch": 0.6213333333333333,
      "grad_norm": 0.00388215109705925,
      "learning_rate": 1.896666666666667e-05,
      "loss": 0.0009,
      "step": 4660
    },
    {
      "epoch": 0.624,
      "grad_norm": 0.005783373024314642,
      "learning_rate": 1.8833333333333335e-05,
      "loss": 0.0453,
      "step": 4680
    },
    {
      "epoch": 0.6266666666666667,
      "grad_norm": 0.011319843120872974,
      "learning_rate": 1.87e-05,
      "loss": 0.0004,
      "step": 4700
    },
    {
      "epoch": 0.6293333333333333,
      "grad_norm": 0.012918134219944477,
      "learning_rate": 1.856666666666667e-05,
      "loss": 0.0018,
      "step": 4720
    },
    {
      "epoch": 0.632,
      "grad_norm": 0.007212408352643251,
      "learning_rate": 1.8433333333333332e-05,
      "loss": 0.0039,
      "step": 4740
    },
    {
      "epoch": 0.6346666666666667,
      "grad_norm": 0.02194163203239441,
      "learning_rate": 1.83e-05,
      "loss": 0.0474,
      "step": 4760
    },
    {
      "epoch": 0.6373333333333333,
      "grad_norm": 0.013479539193212986,
      "learning_rate": 1.8166666666666667e-05,
      "loss": 0.0561,
      "step": 4780
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.012219967320561409,
      "learning_rate": 1.8033333333333336e-05,
      "loss": 0.0366,
      "step": 4800
    },
    {
      "epoch": 0.6426666666666667,
      "grad_norm": 0.011911238543689251,
      "learning_rate": 1.79e-05,
      "loss": 0.0042,
      "step": 4820
    },
    {
      "epoch": 0.6453333333333333,
      "grad_norm": 3.9303526878356934,
      "learning_rate": 1.7766666666666667e-05,
      "loss": 0.0247,
      "step": 4840
    },
    {
      "epoch": 0.648,
      "grad_norm": 0.02981974184513092,
      "learning_rate": 1.7633333333333336e-05,
      "loss": 0.0046,
      "step": 4860
    },
    {
      "epoch": 0.6506666666666666,
      "grad_norm": 0.007502283435314894,
      "learning_rate": 1.75e-05,
      "loss": 0.0196,
      "step": 4880
    },
    {
      "epoch": 0.6533333333333333,
      "grad_norm": 0.008345363661646843,
      "learning_rate": 1.7366666666666668e-05,
      "loss": 0.0022,
      "step": 4900
    },
    {
      "epoch": 0.656,
      "grad_norm": 0.004333626013249159,
      "learning_rate": 1.7233333333333333e-05,
      "loss": 0.0008,
      "step": 4920
    },
    {
      "epoch": 0.6586666666666666,
      "grad_norm": 0.007011059205979109,
      "learning_rate": 1.7100000000000002e-05,
      "loss": 0.0071,
      "step": 4940
    },
    {
      "epoch": 0.6613333333333333,
      "grad_norm": 0.006682990118861198,
      "learning_rate": 1.6966666666666668e-05,
      "loss": 0.0052,
      "step": 4960
    },
    {
      "epoch": 0.664,
      "grad_norm": 0.004310103133320808,
      "learning_rate": 1.6833333333333334e-05,
      "loss": 0.005,
      "step": 4980
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.004276539199054241,
      "learning_rate": 1.6700000000000003e-05,
      "loss": 0.0029,
      "step": 5000
    },
    {
      "epoch": 0.6693333333333333,
      "grad_norm": 0.010725580155849457,
      "learning_rate": 1.6566666666666665e-05,
      "loss": 0.0544,
      "step": 5020
    },
    {
      "epoch": 0.672,
      "grad_norm": 0.015764323994517326,
      "learning_rate": 1.6433333333333334e-05,
      "loss": 0.031,
      "step": 5040
    },
    {
      "epoch": 0.6746666666666666,
      "grad_norm": 0.012716635130345821,
      "learning_rate": 1.63e-05,
      "loss": 0.0105,
      "step": 5060
    },
    {
      "epoch": 0.6773333333333333,
      "grad_norm": 0.01815868355333805,
      "learning_rate": 1.6166666666666665e-05,
      "loss": 0.0005,
      "step": 5080
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.017245130613446236,
      "learning_rate": 1.6033333333333335e-05,
      "loss": 0.002,
      "step": 5100
    },
    {
      "epoch": 0.6826666666666666,
      "grad_norm": 0.008354908786714077,
      "learning_rate": 1.59e-05,
      "loss": 0.0077,
      "step": 5120
    },
    {
      "epoch": 0.6853333333333333,
      "grad_norm": 0.01235391478985548,
      "learning_rate": 1.576666666666667e-05,
      "loss": 0.003,
      "step": 5140
    },
    {
      "epoch": 0.688,
      "grad_norm": 0.004440723452717066,
      "learning_rate": 1.563333333333333e-05,
      "loss": 0.0054,
      "step": 5160
    },
    {
      "epoch": 0.6906666666666667,
      "grad_norm": 0.005681018810719252,
      "learning_rate": 1.55e-05,
      "loss": 0.0273,
      "step": 5180
    },
    {
      "epoch": 0.6933333333333334,
      "grad_norm": 0.005808149464428425,
      "learning_rate": 1.536666666666667e-05,
      "loss": 0.0074,
      "step": 5200
    },
    {
      "epoch": 0.696,
      "grad_norm": 0.013259299099445343,
      "learning_rate": 1.5233333333333332e-05,
      "loss": 0.0313,
      "step": 5220
    },
    {
      "epoch": 0.6986666666666667,
      "grad_norm": 0.005185629706829786,
      "learning_rate": 1.51e-05,
      "loss": 0.0004,
      "step": 5240
    },
    {
      "epoch": 0.7013333333333334,
      "grad_norm": 0.010976606979966164,
      "learning_rate": 1.4966666666666668e-05,
      "loss": 0.0267,
      "step": 5260
    },
    {
      "epoch": 0.704,
      "grad_norm": 0.00731670344248414,
      "learning_rate": 1.4833333333333336e-05,
      "loss": 0.0004,
      "step": 5280
    },
    {
      "epoch": 0.7066666666666667,
      "grad_norm": 0.011090043000876904,
      "learning_rate": 1.47e-05,
      "loss": 0.0023,
      "step": 5300
    },
    {
      "epoch": 0.7093333333333334,
      "grad_norm": 0.013555972836911678,
      "learning_rate": 1.4566666666666667e-05,
      "loss": 0.0222,
      "step": 5320
    },
    {
      "epoch": 0.712,
      "grad_norm": 0.010992670431733131,
      "learning_rate": 1.4433333333333335e-05,
      "loss": 0.008,
      "step": 5340
    },
    {
      "epoch": 0.7146666666666667,
      "grad_norm": 0.011688665486872196,
      "learning_rate": 1.43e-05,
      "loss": 0.023,
      "step": 5360
    },
    {
      "epoch": 0.7173333333333334,
      "grad_norm": 0.01778041571378708,
      "learning_rate": 1.4166666666666668e-05,
      "loss": 0.0045,
      "step": 5380
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.005292629823088646,
      "learning_rate": 1.4033333333333335e-05,
      "loss": 0.0029,
      "step": 5400
    },
    {
      "epoch": 0.7226666666666667,
      "grad_norm": 0.007339450530707836,
      "learning_rate": 1.3900000000000002e-05,
      "loss": 0.0003,
      "step": 5420
    },
    {
      "epoch": 0.7253333333333334,
      "grad_norm": 0.005314749665558338,
      "learning_rate": 1.3766666666666666e-05,
      "loss": 0.0666,
      "step": 5440
    },
    {
      "epoch": 0.728,
      "grad_norm": 0.0032125201541930437,
      "learning_rate": 1.3633333333333334e-05,
      "loss": 0.0049,
      "step": 5460
    },
    {
      "epoch": 0.7306666666666667,
      "grad_norm": 0.004621490370482206,
      "learning_rate": 1.3500000000000001e-05,
      "loss": 0.0264,
      "step": 5480
    },
    {
      "epoch": 0.7333333333333333,
      "grad_norm": 6.341206073760986,
      "learning_rate": 1.3366666666666667e-05,
      "loss": 0.0054,
      "step": 5500
    },
    {
      "epoch": 0.736,
      "grad_norm": 0.014556427486240864,
      "learning_rate": 1.3233333333333334e-05,
      "loss": 0.0301,
      "step": 5520
    },
    {
      "epoch": 0.7386666666666667,
      "grad_norm": 0.011180979199707508,
      "learning_rate": 1.3100000000000002e-05,
      "loss": 0.0125,
      "step": 5540
    },
    {
      "epoch": 0.7413333333333333,
      "grad_norm": 0.45429423451423645,
      "learning_rate": 1.2966666666666669e-05,
      "loss": 0.0016,
      "step": 5560
    },
    {
      "epoch": 0.744,
      "grad_norm": 0.009439057670533657,
      "learning_rate": 1.2833333333333333e-05,
      "loss": 0.0004,
      "step": 5580
    },
    {
      "epoch": 0.7466666666666667,
      "grad_norm": 0.009054740890860558,
      "learning_rate": 1.27e-05,
      "loss": 0.0331,
      "step": 5600
    },
    {
      "epoch": 0.7493333333333333,
      "grad_norm": 0.7319434285163879,
      "learning_rate": 1.2566666666666668e-05,
      "loss": 0.0069,
      "step": 5620
    },
    {
      "epoch": 0.752,
      "grad_norm": 0.00951654277741909,
      "learning_rate": 1.2433333333333335e-05,
      "loss": 0.0013,
      "step": 5640
    },
    {
      "epoch": 0.7546666666666667,
      "grad_norm": 0.008586467243731022,
      "learning_rate": 1.23e-05,
      "loss": 0.0273,
      "step": 5660
    },
    {
      "epoch": 0.7573333333333333,
      "grad_norm": 0.28599822521209717,
      "learning_rate": 1.2166666666666668e-05,
      "loss": 0.0278,
      "step": 5680
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.008901037275791168,
      "learning_rate": 1.2033333333333334e-05,
      "loss": 0.0209,
      "step": 5700
    },
    {
      "epoch": 0.7626666666666667,
      "grad_norm": 0.01002489123493433,
      "learning_rate": 1.19e-05,
      "loss": 0.0081,
      "step": 5720
    },
    {
      "epoch": 0.7653333333333333,
      "grad_norm": 0.006870871875435114,
      "learning_rate": 1.1766666666666667e-05,
      "loss": 0.0002,
      "step": 5740
    },
    {
      "epoch": 0.768,
      "grad_norm": 5.649474143981934,
      "learning_rate": 1.1633333333333334e-05,
      "loss": 0.0476,
      "step": 5760
    },
    {
      "epoch": 0.7706666666666667,
      "grad_norm": 0.006283615715801716,
      "learning_rate": 1.1500000000000002e-05,
      "loss": 0.0031,
      "step": 5780
    },
    {
      "epoch": 0.7733333333333333,
      "grad_norm": 0.006477397866547108,
      "learning_rate": 1.1366666666666667e-05,
      "loss": 0.0126,
      "step": 5800
    },
    {
      "epoch": 0.776,
      "grad_norm": 0.0067358920350670815,
      "learning_rate": 1.1233333333333333e-05,
      "loss": 0.0485,
      "step": 5820
    },
    {
      "epoch": 0.7786666666666666,
      "grad_norm": 0.006316484417766333,
      "learning_rate": 1.11e-05,
      "loss": 0.0142,
      "step": 5840
    },
    {
      "epoch": 0.7813333333333333,
      "grad_norm": 0.007128466852009296,
      "learning_rate": 1.0966666666666666e-05,
      "loss": 0.001,
      "step": 5860
    },
    {
      "epoch": 0.784,
      "grad_norm": 0.008042950183153152,
      "learning_rate": 1.0833333333333334e-05,
      "loss": 0.0003,
      "step": 5880
    },
    {
      "epoch": 0.7866666666666666,
      "grad_norm": 0.026379281654953957,
      "learning_rate": 1.0700000000000001e-05,
      "loss": 0.0062,
      "step": 5900
    },
    {
      "epoch": 0.7893333333333333,
      "grad_norm": 0.007731415331363678,
      "learning_rate": 1.0566666666666668e-05,
      "loss": 0.0017,
      "step": 5920
    },
    {
      "epoch": 0.792,
      "grad_norm": 0.007868088781833649,
      "learning_rate": 1.0433333333333334e-05,
      "loss": 0.0282,
      "step": 5940
    },
    {
      "epoch": 0.7946666666666666,
      "grad_norm": 0.017823372036218643,
      "learning_rate": 1.03e-05,
      "loss": 0.0678,
      "step": 5960
    },
    {
      "epoch": 0.7973333333333333,
      "grad_norm": 0.013684495352208614,
      "learning_rate": 1.0166666666666667e-05,
      "loss": 0.019,
      "step": 5980
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.023557139560580254,
      "learning_rate": 1.0033333333333333e-05,
      "loss": 0.0009,
      "step": 6000
    },
    {
      "epoch": 0.8026666666666666,
      "grad_norm": 0.014222961850464344,
      "learning_rate": 9.900000000000002e-06,
      "loss": 0.0026,
      "step": 6020
    },
    {
      "epoch": 0.8053333333333333,
      "grad_norm": 0.014781760983169079,
      "learning_rate": 9.766666666666667e-06,
      "loss": 0.0005,
      "step": 6040
    },
    {
      "epoch": 0.808,
      "grad_norm": 0.015136059373617172,
      "learning_rate": 9.633333333333335e-06,
      "loss": 0.0029,
      "step": 6060
    },
    {
      "epoch": 0.8106666666666666,
      "grad_norm": 0.010902483016252518,
      "learning_rate": 9.5e-06,
      "loss": 0.0013,
      "step": 6080
    },
    {
      "epoch": 0.8133333333333334,
      "grad_norm": 0.015618519857525826,
      "learning_rate": 9.366666666666666e-06,
      "loss": 0.0168,
      "step": 6100
    },
    {
      "epoch": 0.816,
      "grad_norm": 0.0038551166653633118,
      "learning_rate": 9.233333333333334e-06,
      "loss": 0.0222,
      "step": 6120
    },
    {
      "epoch": 0.8186666666666667,
      "grad_norm": 0.00509985163807869,
      "learning_rate": 9.100000000000001e-06,
      "loss": 0.0005,
      "step": 6140
    },
    {
      "epoch": 0.8213333333333334,
      "grad_norm": 0.012858012691140175,
      "learning_rate": 8.966666666666668e-06,
      "loss": 0.0004,
      "step": 6160
    },
    {
      "epoch": 0.824,
      "grad_norm": 0.01518653891980648,
      "learning_rate": 8.833333333333334e-06,
      "loss": 0.0018,
      "step": 6180
    },
    {
      "epoch": 0.8266666666666667,
      "grad_norm": 0.010248850099742413,
      "learning_rate": 8.7e-06,
      "loss": 0.0154,
      "step": 6200
    },
    {
      "epoch": 0.8293333333333334,
      "grad_norm": 7.749772071838379,
      "learning_rate": 8.566666666666667e-06,
      "loss": 0.0024,
      "step": 6220
    },
    {
      "epoch": 0.832,
      "grad_norm": 0.006611428689211607,
      "learning_rate": 8.433333333333333e-06,
      "loss": 0.0156,
      "step": 6240
    },
    {
      "epoch": 0.8346666666666667,
      "grad_norm": 0.004042390268296003,
      "learning_rate": 8.3e-06,
      "loss": 0.0055,
      "step": 6260
    },
    {
      "epoch": 0.8373333333333334,
      "grad_norm": 0.006711320951581001,
      "learning_rate": 8.166666666666668e-06,
      "loss": 0.016,
      "step": 6280
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.00860087014734745,
      "learning_rate": 8.033333333333335e-06,
      "loss": 0.0003,
      "step": 6300
    },
    {
      "epoch": 0.8426666666666667,
      "grad_norm": 17.195043563842773,
      "learning_rate": 7.9e-06,
      "loss": 0.0604,
      "step": 6320
    },
    {
      "epoch": 0.8453333333333334,
      "grad_norm": 0.011364549398422241,
      "learning_rate": 7.766666666666666e-06,
      "loss": 0.0009,
      "step": 6340
    },
    {
      "epoch": 0.848,
      "grad_norm": 0.007394423708319664,
      "learning_rate": 7.633333333333334e-06,
      "loss": 0.0224,
      "step": 6360
    },
    {
      "epoch": 0.8506666666666667,
      "grad_norm": 0.005035239737480879,
      "learning_rate": 7.5e-06,
      "loss": 0.0004,
      "step": 6380
    },
    {
      "epoch": 0.8533333333333334,
      "grad_norm": 0.0062074181623756886,
      "learning_rate": 7.3666666666666676e-06,
      "loss": 0.0011,
      "step": 6400
    },
    {
      "epoch": 0.856,
      "grad_norm": 0.22013653814792633,
      "learning_rate": 7.233333333333333e-06,
      "loss": 0.0022,
      "step": 6420
    },
    {
      "epoch": 0.8586666666666667,
      "grad_norm": 0.013554647564888,
      "learning_rate": 7.1e-06,
      "loss": 0.0011,
      "step": 6440
    },
    {
      "epoch": 0.8613333333333333,
      "grad_norm": 0.003371128346771002,
      "learning_rate": 6.966666666666667e-06,
      "loss": 0.0003,
      "step": 6460
    },
    {
      "epoch": 0.864,
      "grad_norm": 0.008610052987933159,
      "learning_rate": 6.833333333333333e-06,
      "loss": 0.001,
      "step": 6480
    },
    {
      "epoch": 0.8666666666666667,
      "grad_norm": 0.010244615375995636,
      "learning_rate": 6.700000000000001e-06,
      "loss": 0.0012,
      "step": 6500
    },
    {
      "epoch": 0.8693333333333333,
      "grad_norm": 0.006434333976358175,
      "learning_rate": 6.566666666666667e-06,
      "loss": 0.0109,
      "step": 6520
    },
    {
      "epoch": 0.872,
      "grad_norm": 0.2570180892944336,
      "learning_rate": 6.433333333333334e-06,
      "loss": 0.001,
      "step": 6540
    },
    {
      "epoch": 0.8746666666666667,
      "grad_norm": 0.006349967792630196,
      "learning_rate": 6.300000000000001e-06,
      "loss": 0.0002,
      "step": 6560
    },
    {
      "epoch": 0.8773333333333333,
      "grad_norm": 0.007462471257895231,
      "learning_rate": 6.166666666666667e-06,
      "loss": 0.0287,
      "step": 6580
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.002839926863089204,
      "learning_rate": 6.033333333333334e-06,
      "loss": 0.0056,
      "step": 6600
    },
    {
      "epoch": 0.8826666666666667,
      "grad_norm": 0.010297915898263454,
      "learning_rate": 5.9e-06,
      "loss": 0.0233,
      "step": 6620
    },
    {
      "epoch": 0.8853333333333333,
      "grad_norm": 0.011562426574528217,
      "learning_rate": 5.766666666666667e-06,
      "loss": 0.005,
      "step": 6640
    },
    {
      "epoch": 0.888,
      "grad_norm": 0.005425456445664167,
      "learning_rate": 5.633333333333333e-06,
      "loss": 0.0003,
      "step": 6660
    },
    {
      "epoch": 0.8906666666666667,
      "grad_norm": 0.2615048587322235,
      "learning_rate": 5.506666666666667e-06,
      "loss": 0.0171,
      "step": 6680
    },
    {
      "epoch": 0.8933333333333333,
      "grad_norm": 0.006319893058389425,
      "learning_rate": 5.373333333333333e-06,
      "loss": 0.0017,
      "step": 6700
    },
    {
      "epoch": 0.896,
      "grad_norm": 0.006845329888164997,
      "learning_rate": 5.240000000000001e-06,
      "loss": 0.0102,
      "step": 6720
    },
    {
      "epoch": 0.8986666666666666,
      "grad_norm": 0.008747361600399017,
      "learning_rate": 5.106666666666667e-06,
      "loss": 0.0011,
      "step": 6740
    },
    {
      "epoch": 0.9013333333333333,
      "grad_norm": 0.006792636588215828,
      "learning_rate": 4.973333333333334e-06,
      "loss": 0.008,
      "step": 6760
    },
    {
      "epoch": 0.904,
      "grad_norm": 0.0063718766905367374,
      "learning_rate": 4.84e-06,
      "loss": 0.0175,
      "step": 6780
    },
    {
      "epoch": 0.9066666666666666,
      "grad_norm": 0.005626004189252853,
      "learning_rate": 4.706666666666667e-06,
      "loss": 0.0002,
      "step": 6800
    },
    {
      "epoch": 0.9093333333333333,
      "grad_norm": 0.00504717044532299,
      "learning_rate": 4.573333333333333e-06,
      "loss": 0.0055,
      "step": 6820
    },
    {
      "epoch": 0.912,
      "grad_norm": 0.009680991061031818,
      "learning_rate": 4.440000000000001e-06,
      "loss": 0.058,
      "step": 6840
    },
    {
      "epoch": 0.9146666666666666,
      "grad_norm": 0.011403066106140614,
      "learning_rate": 4.306666666666667e-06,
      "loss": 0.0019,
      "step": 6860
    },
    {
      "epoch": 0.9173333333333333,
      "grad_norm": 0.012116145342588425,
      "learning_rate": 4.173333333333333e-06,
      "loss": 0.0003,
      "step": 6880
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.016981402412056923,
      "learning_rate": 4.04e-06,
      "loss": 0.0427,
      "step": 6900
    },
    {
      "epoch": 0.9226666666666666,
      "grad_norm": 0.01014030072838068,
      "learning_rate": 3.906666666666667e-06,
      "loss": 0.0019,
      "step": 6920
    },
    {
      "epoch": 0.9253333333333333,
      "grad_norm": 0.02610953524708748,
      "learning_rate": 3.7733333333333338e-06,
      "loss": 0.0021,
      "step": 6940
    },
    {
      "epoch": 0.928,
      "grad_norm": 0.008491416461765766,
      "learning_rate": 3.6400000000000003e-06,
      "loss": 0.0117,
      "step": 6960
    },
    {
      "epoch": 0.9306666666666666,
      "grad_norm": 2.365236282348633,
      "learning_rate": 3.5066666666666673e-06,
      "loss": 0.0243,
      "step": 6980
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 0.015811195597052574,
      "learning_rate": 3.3733333333333334e-06,
      "loss": 0.0152,
      "step": 7000
    },
    {
      "epoch": 0.936,
      "grad_norm": 0.017893876880407333,
      "learning_rate": 3.24e-06,
      "loss": 0.0005,
      "step": 7020
    },
    {
      "epoch": 0.9386666666666666,
      "grad_norm": 0.0034858386497944593,
      "learning_rate": 3.106666666666667e-06,
      "loss": 0.0006,
      "step": 7040
    },
    {
      "epoch": 0.9413333333333334,
      "grad_norm": 0.009903883561491966,
      "learning_rate": 2.9733333333333334e-06,
      "loss": 0.0004,
      "step": 7060
    },
    {
      "epoch": 0.944,
      "grad_norm": 0.008760233409702778,
      "learning_rate": 2.8400000000000003e-06,
      "loss": 0.0147,
      "step": 7080
    },
    {
      "epoch": 0.9466666666666667,
      "grad_norm": 0.008736759424209595,
      "learning_rate": 2.706666666666667e-06,
      "loss": 0.018,
      "step": 7100
    },
    {
      "epoch": 0.9493333333333334,
      "grad_norm": 0.01563042216002941,
      "learning_rate": 2.5733333333333334e-06,
      "loss": 0.0004,
      "step": 7120
    },
    {
      "epoch": 0.952,
      "grad_norm": 0.011739139445126057,
      "learning_rate": 2.4400000000000004e-06,
      "loss": 0.0152,
      "step": 7140
    },
    {
      "epoch": 0.9546666666666667,
      "grad_norm": 0.004433258902281523,
      "learning_rate": 2.306666666666667e-06,
      "loss": 0.0053,
      "step": 7160
    },
    {
      "epoch": 0.9573333333333334,
      "grad_norm": 0.03813469037413597,
      "learning_rate": 2.1733333333333334e-06,
      "loss": 0.0165,
      "step": 7180
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.009601863101124763,
      "learning_rate": 2.0400000000000004e-06,
      "loss": 0.0061,
      "step": 7200
    },
    {
      "epoch": 0.9626666666666667,
      "grad_norm": 0.02189388871192932,
      "learning_rate": 1.9066666666666667e-06,
      "loss": 0.0006,
      "step": 7220
    },
    {
      "epoch": 0.9653333333333334,
      "grad_norm": 0.015251297503709793,
      "learning_rate": 1.7733333333333334e-06,
      "loss": 0.0252,
      "step": 7240
    },
    {
      "epoch": 0.968,
      "grad_norm": 0.016579732298851013,
      "learning_rate": 1.6400000000000002e-06,
      "loss": 0.0005,
      "step": 7260
    },
    {
      "epoch": 0.9706666666666667,
      "grad_norm": 0.03160791099071503,
      "learning_rate": 1.5066666666666667e-06,
      "loss": 0.0003,
      "step": 7280
    },
    {
      "epoch": 0.9733333333333334,
      "grad_norm": 0.007784998510032892,
      "learning_rate": 1.3733333333333335e-06,
      "loss": 0.0004,
      "step": 7300
    },
    {
      "epoch": 0.976,
      "grad_norm": 0.25304073095321655,
      "learning_rate": 1.24e-06,
      "loss": 0.0118,
      "step": 7320
    },
    {
      "epoch": 0.9786666666666667,
      "grad_norm": 0.01296485960483551,
      "learning_rate": 1.1066666666666667e-06,
      "loss": 0.0264,
      "step": 7340
    },
    {
      "epoch": 0.9813333333333333,
      "grad_norm": 0.011701542884111404,
      "learning_rate": 9.733333333333335e-07,
      "loss": 0.002,
      "step": 7360
    },
    {
      "epoch": 0.984,
      "grad_norm": 0.009418890811502934,
      "learning_rate": 8.4e-07,
      "loss": 0.0004,
      "step": 7380
    },
    {
      "epoch": 0.9866666666666667,
      "grad_norm": 0.005257809534668922,
      "learning_rate": 7.066666666666666e-07,
      "loss": 0.0564,
      "step": 7400
    },
    {
      "epoch": 0.9893333333333333,
      "grad_norm": 0.003164546797052026,
      "learning_rate": 5.733333333333334e-07,
      "loss": 0.0003,
      "step": 7420
    },
    {
      "epoch": 0.992,
      "grad_norm": 0.008739838376641273,
      "learning_rate": 4.4e-07,
      "loss": 0.0294,
      "step": 7440
    },
    {
      "epoch": 0.9946666666666667,
      "grad_norm": 0.006790022365748882,
      "learning_rate": 3.066666666666667e-07,
      "loss": 0.0352,
      "step": 7460
    },
    {
      "epoch": 0.9973333333333333,
      "grad_norm": 0.004243955947458744,
      "learning_rate": 1.7333333333333332e-07,
      "loss": 0.0004,
      "step": 7480
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.011563793756067753,
      "learning_rate": 4e-08,
      "loss": 0.0621,
      "step": 7500
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.9955,
      "eval_f1": 0.9954988146272324,
      "eval_loss": 0.02408156357705593,
      "eval_precision": 0.9955158103030013,
      "eval_recall": 0.9955,
      "eval_runtime": 144.4788,
      "eval_samples_per_second": 415.286,
      "eval_steps_per_second": 25.955,
      "step": 7500
    }
  ],
  "logging_steps": 20,
  "max_steps": 7500,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 7948327403520000.0,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
